import streamlit as st
from huggingface_hub import InferenceClient
import requests
from docx import Document
from typing import List
import textwrap

# ---------------- Configuraci√≥n por defecto ---------------- #

DEFAULT_MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2"
DEFAULT_HF_TOKEN = st.secrets.get("HF_TOKEN", "")  # Si no se configur√≥, queda vac√≠o

# ---------------- Cliente Hugging Face ---------------- #

def get_client(token: str | None = None, model_id: str = DEFAULT_MODEL_ID):
    token = token or DEFAULT_HF_TOKEN
    if not token:
        return None  # Modo simple
    return InferenceClient(model=model_id, token=token)

def hf_generate(client, prompt: str, max_tokens: int = 1024, temperature: float = 0.3) -> str:
    if client is None:  # Modo simple
        return simple_llm(prompt)
    return client.text_generation(
        prompt,
        max_new_tokens=max_tokens,
        temperature=temperature,
        top_p=0.9,
        repetition_penalty=1.1,
    )

# ---------------- Modo simple (fallback) ---------------- #

def simple_paragraph(text: str) -> str:
    wrapped = textwrap.fill(text, width=100)
    return wrapped

def simple_llm(prompt: str) -> str:
    placeholders = [
        "Se resaltar√° la trascendencia del fen√≥meno planteado y la pertinencia cient√≠fica de abordarlo.",
        "Se describir√° la magnitud del problema a escala global, regional y nacional, destacando sus repercusiones sanitarias y econ√≥micas.",
        "Se sintetizar√°n las principales brechas de conocimiento detectadas en la literatura, subrayando la necesidad de nuevos estudios.",
        "La investigaci√≥n contribuir√° al Objetivo de Desarrollo Sostenible relacionado con la salud y el bienestar.",
        "¬øEn qu√© medida el fen√≥meno descrito se asociar√° con las variables seleccionadas en el contexto propuesto?",
        "Desde el punto de vista te√≥rico, el estudio profundizar√° en los modelos conceptuales vigentes y propondr√° nuevas perspectivas.",
        "En el plano pr√°ctico, los hallazgos orientar√°n intervenciones basadas en evidencia para mejorar la situaci√≥n analizada.",
        "Metodol√≥gicamente, se emplear√° un dise√±o robusto que garantizar√° la validez y confiabilidad de los resultados obtenidos.",
        "El beneficio social radicar√° en la generaci√≥n de conocimiento aplicable que favorecer√° la calidad de vida de la poblaci√≥n implicada."
    ]
    return "\n\n".join(simple_paragraph(p) for p in placeholders)

# ---------------- Funciones principales ---------------- #

def generate_introduction(client, title: str, objective: str, word_target: int = 4500) -> str:
    prompt = f"""
Redacta una introducci√≥n de tesis acad√©mica con las siguientes caracter√≠sticas:
‚Ä¢ Extensi√≥n m√≠nima: {word_target} palabras (‚âà 9 p√°ginas A4).
‚Ä¢ Prosa, sin subt√≠tulos, tercera persona, tiempo futuro del modo indicativo.
‚Ä¢ M√°ximo 10 l√≠neas por p√°rrafo.
‚Ä¢ Secuencia de p√°rrafos exacta:
  1‚Äë2‚ÄØp Importancia + plausibilidad del problema.
  1‚Äë2‚ÄØp Impacto (mundo, Latinoam√©rica, Per√∫).
  1‚Äë2‚ÄØp Vac√≠os de literatura.
  1‚ÄØp Contribuci√≥n a ODS.
  1‚ÄØp Pregunta de investigaci√≥n (interrogativa).
  1‚ÄØp Justificaci√≥n te√≥rica.
  1‚ÄØp Justificaci√≥n pr√°ctica.
  1‚ÄØp Justificaci√≥n metodol√≥gica.
  1‚ÄØp Justificaci√≥n social.
Al final escribe la l√≠nea EXACTA "===ANTECEDENTES===".
T√≠tulo: {title}
Objetivo general: {objective}
"""
    return hf_generate(client, prompt, max_tokens=4096)

def paraphrase_abstract(client, abstract: str, word_count: int = 130) -> str:
    if client is None:
        return simple_paragraph("Resumen disponible para consulta; se presentar√° de forma sint√©tica y libre de plagio en la versi√≥n final.")
    prompt = (
        f"Parafrasea en espa√±ol acad√©mico el siguiente resumen en aproximadamente {word_count} palabras, evitando plagio:\n{abstract}"
    )
    return hf_generate(client, prompt, max_tokens=256, temperature=0.4)

def search_pubmed(query: str, n: int = 10) -> List[dict]:
    base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
    ids = requests.get(base + "esearch.fcgi", params={"db": "pubmed", "term": query, "retmax": n, "retmode": "json"}).json()["esearchresult"]["idlist"]
    items = []
    for pmid in ids:
        xml = requests.get(base + "efetch.fcgi", params={"db": "pubmed", "id": pmid, "retmode": "xml"}).text
        import re, html
        abstract = re.search(r"<AbstractText.*?>(.*?)</AbstractText>", xml, re.S)
        if not abstract:
            continue
        authors = re.findall(r"<LastName>(.*?)</LastName>.*?<Initials>(.*?)</Initials>", xml, re.S)
        year = re.search(r"<PubDate>.*?<Year>(\d{4})</Year>", xml, re.S)
        if authors:
            first = f"{authors[0][0]}, {authors[0][1][0]}."
            cite = first + (" et al." if len(authors) > 3 else "")
        else:
            cite = "Autor desconocido"
        items.append({
            "cite": f"{cite} ({year.group(1) if year else 's.f.'})",
            "abstract": html.unescape(abstract.group(1))
        })
    return items[:n]

def build_antecedents(client, pubs: List[dict]) -> str:
    return "\n\n".join(
        f"{p['cite']} {paraphrase_abstract(client, p['abstract'])}" for p in pubs
    ) or "Sin referencias disponibles."

def generate_theoretical_bases(client, title: str, objective: str) -> str:
    if client is None:
        return simple_paragraph("Se desarrollar√°n los fundamentos conceptuales que sustentan la relaci√≥n entre las variables propuestas y se explicar√° el marco te√≥rico que guiar√° el an√°lisis.")
    prompt = (
        "Redacta las bases te√≥ricas pertinentes a la pregunta de investigaci√≥n, incluyendo enfoques conceptuales y variables clave. "
        "Prosa acad√©mica, tercera persona, futuro indicativo.\n"
        f"T√≠tulo: {title}\nObjetivo general: {objective}"
    )
    return hf_generate(client, prompt, max_tokens=1024)

def generate_hypotheses(client, objective: str) -> str:
    if client is None:
        return simple_paragraph("Hip√≥tesis de investigaci√≥n y estad√≠sticas ser√°n formuladas relacionando las variables primarias para demostrar la direcci√≥n y fuerza del efecto esperado.")
    prompt = (
        "Formula la hip√≥tesis de investigaci√≥n y las hip√≥tesis estad√≠sticas (nula y alternativa) basadas en el siguiente objetivo general.\n"
        f"{objective}"
    )
    return hf_generate(client, prompt, max_tokens=256)

def build_docx(intro: str, antecedentes: str, bases: str, hyps: str) -> bytes:
    doc = Document()
    doc.add_heading("Proyecto de Tesis ‚Äì Secciones Generadas", 1)
    doc.add_heading("Introducci√≥n", 2)
    for p in intro.split("\n"):
        doc.add_paragraph(p)
    doc.add_page_break()

    doc.add_heading("Antecedentes", 2)
    for p in antecedentes.split("\n"):
        doc.add_paragraph(p)
    doc.add_page_break()

    doc.add_heading("Bases Te√≥ricas", 2)
    for p in bases.split("\n"):
        doc.add_paragraph(p)
    doc.add_page_break()

    doc.add_heading("Hip√≥tesis", 2)
    for p in hyps.split("\n"):
        doc.add_paragraph(p)

    from io import BytesIO
    buffer = BytesIO()
    doc.save(buffer)
    return buffer.getvalue()

# ---------------- Interfaz Streamlit ---------------- #

st.set_page_config(page_title="Generador de Introducciones de Tesis", layout="wide")
st.title("üìù Generador Autom√°tico de Introducciones de Tesis ‚Äì Modo LLM o Simple")

with st.sidebar:
    st.header("Configuraci√≥n")
    hf_token = st.text_input("Hugging Face API Token (opcional)", type="password", value="")
    model_id = st.text_input("ID del modelo (HF Hub)", value=DEFAULT_MODEL_ID)
    title_input = st.text_input("T√≠tulo de la Investigaci√≥n")
    objective_input = st.text_area("Objetivo General")
    generate_btn = st.button("Generar Introducci√≥n")

if generate_btn:
    client = get_client(hf_token, model_id)

    with st.spinner("Generando introducci√≥n‚Ä¶"):
        intro = generate_introduction(client, title_input, objective_input)
    st.subheader("Introducci√≥n")
    st.markdown(intro)

    with st.spinner("Buscando antecedentes en PubMed‚Ä¶"):
        pubs = search_pubmed(title_input or objective_input) if (title_input or objective_input) else []
        antecedentes = build_antecedents(client, pubs)
    st.subheader("Antecedentes")
    st.markdown(antecedentes)

    with st.spinner("Generando bases te√≥ricas‚Ä¶"):
        bases = generate_theoretical_bases(client, title_input, objective_input)
    st.subheader("Bases Te√≥ricas")
    st.markdown(bases)

    with st.spinner("Generando hip√≥tesis‚Ä¶"):
        hyps = generate_hypotheses(client, objective_input)
    st.subheader("Hip√≥tesis")
    st.markdown(hyps)

    st.success("¬°Secciones generadas! Puedes copiar el texto o exportar a Word.")

    docx_bytes = build_docx(intro, antecedentes, bases, hyps)
    st.download_button(
        label="Descargar Word (.docx)",
        data=docx_bytes,
        file_name="proyecto_tesis_generado.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )
